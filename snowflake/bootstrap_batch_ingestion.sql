-- Bootstrap Snowflake objects for batch data ingestion DAG (Simplified)
-- Instructions:
-- 1) Replace the placeholders in ALL_CAPS with your values or run in a worksheet after setting:
--      USE ROLE YOUR_ROLE;
--      USE WAREHOUSE YOUR_WAREHOUSE;
-- 2) Execute this script once to create database/schema/tables.
-- 3) After this, the Airflow DAG should work properly.

-- Set your context (optional if already selected in UI)
-- USE ROLE YOUR_ROLE;
-- USE WAREHOUSE YOUR_WAREHOUSE;

-- Replace these names or keep as-is
-- CREATE DATABASE IF NOT EXISTS DB_T0;
USE DATABASE DB_T0;

CREATE SCHEMA IF NOT EXISTS SC_RAW;
USE SCHEMA SC_RAW;

-- 1. Simple table for PostgreSQL data (from Kafka sink)
CREATE TABLE IF NOT EXISTS TRANSACTIONS_POSTGRES (
    TX_ID STRING PRIMARY KEY,
    USER_ID INTEGER,
    AMOUNT NUMBER(18,2),
    CURRENCY STRING,
    MERCHANT STRING,
    CATEGORY STRING,
    TIMESTAMP TIMESTAMP_NTZ,
    INGESTED_AT TIMESTAMP_NTZ,
    AIRFLOW_INGESTED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP
);

-- 2. Simple table for CSV batch data
CREATE TABLE IF NOT EXISTS TRANSACTIONS_BATCH (
    TX_ID STRING PRIMARY KEY,
    USER_ID INTEGER,
    AMOUNT NUMBER(18,2),
    CURRENCY STRING,
    MERCHANT STRING,
    CATEGORY STRING,
    TIMESTAMP TIMESTAMP_NTZ,
    SOURCE_FILE STRING,  -- filename that was processed
    INGESTED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP
);

-- 3. Internal stage for CSV uploads (dev)
CREATE STAGE IF NOT EXISTS STG_TRANSACTIONS
  FILE_FORMAT = (
    TYPE = CSV
    SKIP_HEADER = 1
    FIELD_OPTIONALLY_ENCLOSED_BY='"'
  );

-- Optional: grants (adjust ROLE/USER/SHARE as needed)
-- GRANT USAGE ON DATABASE DB_T0 TO ROLE YOUR_ROLE;
-- GRANT USAGE ON SCHEMA DB_T0.SC_RAW TO ROLE YOUR_ROLE;
-- GRANT SELECT, INSERT, UPDATE, DELETE ON TABLE TRANSACTIONS_POSTGRES TO ROLE YOUR_ROLE;
-- GRANT SELECT, INSERT, UPDATE, DELETE ON TABLE TRANSACTIONS_BATCH TO ROLE YOUR_ROLE;
-- GRANT USAGE ON STAGE STG_TRANSACTIONS TO ROLE YOUR_ROLE;

-- Verify tables were created
SHOW TABLES IN SCHEMA SC_RAW;
