# PostgreSQL Docker Setup

A simple PostgreSQL database setup using Docker for learning and development.

## Quick Start

### 1. Start PostgreSQL

```bash
# Start the database
docker-compose up -d

# Check if the service is running
docker-compose ps
```

### 2. Access Your Database

**PostgreSQL Database:**
- Host: `localhost`
- Port: `5432`
- Database: `postgres` (or your custom database name)
- Username: `postgres` (or your custom username)
- Password: `your_password` (from your .env file)

### 3. Connect with DBeaver

1. Download and install DBeaver Community from https://dbeaver.io/download/
2. Open DBeaver and click "New Database Connection"
3. Select PostgreSQL from the database list
4. Fill in connection details:
   - Host: `localhost`
   - Port: `5432`
   - Database: `postgres` (or your custom database name)
   - Username: `postgres` (or your custom username)
   - Password: `your_password` (from your .env file)
5. Test the connection and click "Finish"

## Dependency Management with uv

This project uses `uv` for dependency management. Follow these steps to manage dependencies and generate a `requirements.txt` file for Docker builds:

### 1. Install uv

Make sure you have `uv` installed on your local machine. You can install it via pip:

```bash
pip install uv
```

### 2. Export Dependencies

Use `uv` to export your dependencies to a `requirements.txt` file:

```bash
uv export --format requirements-txt --no-dev > requirements.txt
```

### 3. Docker Build

The Dockerfile is set up to use the `requirements.txt` file for installing dependencies:

```dockerfile
# Copy requirements.txt generated by uv
COPY requirements.txt ./

# Install dependencies from requirements.txt
RUN pip install --no-cache-dir -r requirements.txt
```

This approach ensures that all dependencies are managed consistently and installed reliably in the Docker environment.

## Useful Commands

```bash
# Start database
docker-compose up -d

# Stop database
docker-compose down

# View logs
docker-compose logs postgres

# Access PostgreSQL directly via command line
docker exec -it postgres_db psql -U postgres -d postgres

# Remove everything (including data)
docker-compose down -v
```

## Environment Configuration

This project uses environment variables for configuration. Create a `.env` file in the root directory with your specific settings:

```bash
# Example .env file
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_secure_password
POSTGRES_DB=postgres
POSTGRES_PORT=5432
```

The `.env` file is already in `.gitignore` to keep your credentials secure.

## Learning PostgreSQL

### Basic SQL Commands to Try

```sql
-- List all databases
\l

-- Connect to a database
\c database_name

-- List all tables
\dt

-- Describe a table
\d table_name

-- Basic SELECT query
SELECT * FROM table_name LIMIT 10;

-- Create a simple table
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100),
    email VARCHAR(255),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Insert data
INSERT INTO users (name, email) VALUES ('John Doe', 'john@example.com');

-- Query data
SELECT * FROM users;
```

## Troubleshooting

- If port 5432 is already in use, change the port mapping in `docker-compose.yml`
- If you need to reset the database, run `docker-compose down -v` and then `docker-compose up -d`
- Check container logs with `docker-compose logs postgres`

## Batch Data Ingestion DAG

The project includes a comprehensive Airflow DAG for batch data ingestion that handles two independent data sources:

### Overview

**DAG Name**: `batch_data_ingestion_dag`
**Schedule**: Every 15 minutes
**Purpose**: Synchronize data from PostgreSQL (with CDC) and CSV files to Snowflake

### Tasks

1. **PostgreSQL to Snowflake CDC Task** (`postgres_to_snowflake_cdc`)
   - Reads new/updated records from PostgreSQL `transactions_sink` table
   - Implements Change Data Capture (CDC) using timestamp tracking
   - Lands data in Snowflake `TRANSACTIONS_CDC` table
   - Tracks sync history in `TRANSACTIONS_CDC_SYNC_LOG` table

2. **CSV to Snowflake Batch Task** (`csv_to_snowflake_batch`)
   - Processes CSV files from `data/incoming/` folder
   - Lands data in Snowflake `TRANSACTIONS_BATCH` table
   - Tracks source file information
   - Independent of PostgreSQL task

### Setup Instructions

#### 1. Create Snowflake Tables

Run the bootstrap script in Snowflake:

```sql
-- Execute in Snowflake worksheet
-- Set your role and warehouse first
USE ROLE YOUR_ROLE;
USE WAREHOUSE YOUR_WAREHOUSE;

-- Run the bootstrap script
-- Copy content from snowflake/bootstrap_batch_ingestion.sql
```

This creates:
- `TRANSACTIONS_CDC` - for PostgreSQL CDC data
- `TRANSACTIONS_BATCH` - for CSV batch data
- `TRANSACTIONS_CDC_SYNC_LOG` - for CDC tracking
- `V_TRANSACTIONS_UNIFIED` - unified view of all data

#### 2. Set Up PostgreSQL Connection

The DAG needs to connect to the kafka PostgreSQL instance. Run:

```bash
# From project root
./scripts/setup_airflow_postgres_connection.sh
```

This creates the `postgres_kafka_default` connection in Airflow.

#### 3. Verify Snowflake Connection

Ensure the `snowflake_default` connection is already set up:

```bash
# From project root
./scripts/setup_airflow_snowflake_connection.sh
```

#### 4. Start the DAG

The DAG will automatically start running every 15 minutes once:
- All connections are configured
- Snowflake tables are created
- Airflow is running

### Data Flow

```
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│   PostgreSQL    │    │     Airflow      │    │    Snowflake    │
│ transactions_sink│───▶│  CDC Task       │───▶│ TRANSACTIONS_CDC│
└─────────────────┘    └──────────────────┘    └─────────────────┘
                                │
                                ▼
                       ┌──────────────────┐    ┌─────────────────┐
                       │  CSV Task        │───▶│TRANSACTIONS_BATCH│
                       └──────────────────┘    └─────────────────┘
                                │
                                ▼
                       ┌──────────────────┐
                       │data/incoming/    │
                       │*.csv files       │
                       └──────────────────┘
```

### CDC Implementation

The PostgreSQL task implements simple CDC by:
- Tracking the last sync timestamp in `TRANSACTIONS_CDC_SYNC_LOG`
- Querying only records with `ingested_at > last_sync_timestamp`
- Recording sync metadata for audit trails
- Handling incremental updates efficiently

### Monitoring

- **Airflow UI**: Monitor task execution and logs
- **Snowflake**: Query sync logs and data tables
- **Logs**: Check Airflow task logs for detailed execution info

### Tables Schema

#### TRANSACTIONS_CDC
```sql
CREATE TABLE TRANSACTIONS_CDC (
    TX_ID STRING PRIMARY KEY,
    USER_ID INTEGER,
    AMOUNT NUMBER(18,2),
    CURRENCY STRING,
    MERCHANT STRING,
    CATEGORY STRING,
    TIMESTAMP TIMESTAMP_NTZ,
    INGESTED_AT TIMESTAMP_NTZ,
    SOURCE_SYSTEM STRING,
    SYNC_TIMESTAMP TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP
);
```

#### TRANSACTIONS_BATCH
```sql
CREATE TABLE TRANSACTIONS_BATCH (
    TX_ID STRING PRIMARY KEY,
    USER_ID INTEGER,
    AMOUNT NUMBER(18,2),
    CURRENCY STRING,
    MERCHANT STRING,
    CATEGORY STRING,
    TIMESTAMP TIMESTAMP_NTZ,
    SOURCE_FILE STRING,
    INGESTED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP
);
```

### Troubleshooting

1. **Connection Issues**: Verify both PostgreSQL and Snowflake connections in Airflow
2. **Missing Tables**: Ensure Snowflake bootstrap script has been executed
3. **Permission Errors**: Check Snowflake role permissions for the tables
4. **Data Type Mismatches**: Verify CSV column types match expected schema
