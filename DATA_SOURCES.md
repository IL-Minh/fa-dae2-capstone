# Data Sources

## 📊 **Overview**

This project simulates a real-world data pipeline with three distinct data sources that feed into a unified dimensional model. Each source represents different data ingestion patterns and business scenarios.

## 🔄 **Data Sources**

### **1. Batch Transaction Data (CSV)**
- **Origin**: Monthly CSV files generated by `generate_monthly_data.py`
- **Format**: CSV files in `data/incoming/transactions_YYYY_MM.csv`
- **Update Frequency**: Monthly batches
- **Data Freshness**: End-of-month processing
- **Volume**: 500+ transactions per month
- **Granularity**: Transaction-level
- **Time Range**: Historical (multiple months)
- **Quality**: High (financial data, consistent structure)
- **Fields**: tx_id, user_id, amount, currency, merchant, category, timestamp, source_file
- **Business Context**: Monthly financial reporting, historical analysis, compliance

### **2. Real-time Transaction Data (Kafka Stream)**
- **Origin**: Simulated real-time transactions via `kafka_producer_faker.py`
- **Format**: JSON messages on Kafka topic "transactions"
- **Update Frequency**: Continuous stream (every 10 seconds)
- **Data Freshness**: Real-time (within seconds)
- **Volume**: Continuous flow
- **Granularity**: Transaction-level
- **Time Range**: Current (live stream)
- **Quality**: High (simulated, consistent structure)
- **Fields**: tx_id, user_id, amount, currency, merchant, category, timestamp, source_system, transaction_type, processing_time_ms
- **Business Context**: Live monitoring, fraud detection, real-time analytics

### **3. User Profile Data (CSV)**
- **Origin**: Monthly CSV files generated by `generate_monthly_data.py`
- **Format**: CSV files in `data/incoming/user_registrations_YYYY_MM.csv`
- **Update Frequency**: Monthly batches
- **Data Freshness**: End-of-month processing
- **Volume**: 100+ users per month
- **Granularity**: User-level
- **Time Range**: Historical (multiple months)
- **Quality**: Medium (self-reported, demographic data)
- **Fields**: user_id, first_name, last_name, email, age, income_bracket, customer_tier, risk_profile, city, state, country, registration_date, preferred_categories, is_active, source_system
- **Business Context**: Customer segmentation, personalization, risk assessment

## ⚠️ **Important Design Note: Circular Dependency in Fake Data Generation**

### **The Pattern**
There exists a **circular dependency** in our fake data generation that's important to understand:

1. **`generate_monthly_data.py`** creates user registration CSV files
2. **`kafka_producer_faker.py`** reads those CSV files to get user IDs
3. **Kafka producer** generates streaming transactions using those same user IDs
4. **Both sources** (batch CSV + streaming Kafka) now reference the same users

### **Why This Exists**
- **Demo purposes**: Ensures referential integrity for dimensional modeling
- **Realistic simulation**: Shows how different data sources can share common entities
- **Testing**: Validates that our dimensional model can handle unified user data

### **Real-World Equivalent**
In production, this would typically be handled by:
- **Master Data Management (MDM)** systems
- **Identity resolution** services
- **Data governance** processes
- **Cross-system** user ID synchronization

### **Current Implementation**
```python
# In kafka_producer_faker.py
def load_user_ids_from_csv() -> list[str]:
    """Load user IDs from the generated user registration CSV files"""
    # This creates the circular dependency for demo purposes
    user_files = list(incoming_dir.glob("user_registrations_*.csv"))
    df = pl.read_csv(latest_file)
    user_ids = df["user_id"].to_list()
    return user_ids
```

### **Alternative Approaches for Production**
1. **Separate user ID pools** for each source
2. **Real-time user registration** events via Kafka
3. **Database lookups** for user validation
4. **API calls** to user management systems

## 🔗 **Data Integration Challenges**

### **User ID Mismatch**
- **Challenge**: Ensuring user IDs exist across all sources
- **Current Solution**: Circular dependency in fake data generation
- **Production Solution**: Master data management and identity resolution

### **Update Frequency Mismatch**
- **Challenge**: Batch (monthly) vs real-time (continuous) data
- **Solution**: Incremental processing and real-time streaming

### **Data Quality Variations**
- **Challenge**: Different validation rules per source
- **Solution**: Unified data quality framework in dbt

## 🏗️ **Data Flow Architecture**

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Batch CSV     │    │  Real-time      │    │  User Profile   │
│  Transactions   │    │  Transactions   │    │      CSV        │
│                 │    │                 │    │                 │
│ Generated by    │    │ Generated by    │    │ Generated by    │
│ monthly_data.py │    │ kafka_producer  │    │ monthly_data.py │
│                 │    │ (reads CSV)     │    │                 │
└─────────┬───────┘    └─────────┬───────┘    └─────────┬───────┘
          │                       │                       │
          │                       │                       │
          ▼                       ▼                       ▼
┌─────────────────────────────────────────────────────────────────┐
│                    dbt Transformation Layer                      │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐ │
│  │   Staging       │  │   Intermediate  │  │     Marts       │ │
│  │   Models        │  │     Models      │  │     Models      │ │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘ │
└─────────────────────────────────────────────────────────────────┘
          │                       │                       │
          │                       │                       │
          ▼                       ▼                       ▼
┌─────────────────────────────────────────────────────────────────┐
│                    Snowflake Data Warehouse                     │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐ │
│  │ fct_transactions│  │fct_daily_trans  │  │ dim_users       │ │
│  │ (Unified)       │  │ (Aggregated)    │  │ (Conformed)     │ │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘ │
└─────────────────────────────────────────────────────────────────┘
```

## 📈 **Data Quality & Governance**

### **Quality Metrics**
- **Completeness**: Required fields not null
- **Accuracy**: Amount ranges, valid categories, valid dates
- **Consistency**: User IDs match across sources
- **Timeliness**: Data freshness indicators

### **Governance**
- **Data lineage**: Track data from source to consumption
- **Change management**: Version control for model changes
- **Access control**: Role-based permissions by business unit
- **Audit trail**: Track data modifications and access

## 🎯 **Usage Examples**

### **Demo Setup**
```bash
# 1. Generate user and transaction data
uv run scripts/generate_monthly_data.py --year 2025 --month 8 --users 100 --transactions 500

# 2. Start Kafka producer (will use same user IDs)
uv run kafka/kafka_producer_faker.py

# 3. Run Airflow DAG to ingest data
# 4. Build dbt dimensional model
uv run dbt build --project-dir dbt_sf
```

### **Business Questions Answered**
- Which users have both batch and streaming transactions?
- How do spending patterns differ between data sources?
- What's the user journey across different transaction types?
- Which data sources contribute most to user value?

## 📚 **References**

- [Data Integration Patterns](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/)
- [Master Data Management](https://www.gartner.com/en/information-technology/glossary/master-data-management-mdm)
- [Identity Resolution](https://www.snowflake.com/blog/identity-resolution-data-science/)
