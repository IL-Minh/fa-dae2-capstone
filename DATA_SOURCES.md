# Data Sources

## ğŸ“Š **Overview**

This project simulates a real-world data pipeline with three distinct data sources that feed into a unified dimensional model. Each source represents different data ingestion patterns and business scenarios.

## ğŸ”„ **Data Sources**

### **1. Batch Transaction Data (CSV)**
- **Origin**: Monthly CSV files generated by `generate_monthly_data.py`
- **Format**: CSV files in `data/incoming/transactions_YYYY_MM.csv`
- **Update Frequency**: Monthly batches
- **Data Freshness**: End-of-month processing
- **Volume**: 500+ transactions per month
- **Granularity**: Transaction-level
- **Time Range**: Historical (multiple months)
- **Quality**: High (financial data, consistent structure)
- **Fields**: tx_id, user_id, amount, currency, merchant, category, timestamp, source_file
- **Business Context**: Monthly financial reporting, historical analysis, compliance

### **2. Real-time Transaction Data (Kafka Stream)**
- **Origin**: Simulated real-time transactions via `kafka_producer_faker.py`
- **Format**: JSON messages on Kafka topic "transactions"
- **Update Frequency**: Continuous stream (every 10 seconds)
- **Data Freshness**: Real-time (within seconds)
- **Volume**: Continuous flow
- **Granularity**: Transaction-level
- **Time Range**: Current (live stream)
- **Quality**: High (simulated, consistent structure)
- **Fields**: tx_id, user_id, amount, currency, merchant, category, timestamp, source_system, transaction_type, processing_time_ms
- **Business Context**: Live monitoring, fraud detection, real-time analytics

### **3. User Profile Data (CSV)**
- **Origin**: Monthly CSV files generated by `generate_monthly_data.py`
- **Format**: CSV files in `data/incoming/user_registrations_YYYY_MM.csv`
- **Update Frequency**: Monthly batches
- **Data Freshness**: End-of-month processing
- **Volume**: 100+ users per month
- **Granularity**: User-level
- **Time Range**: Historical (multiple months)
- **Quality**: Medium (self-reported, demographic data)
- **Fields**: user_id, first_name, last_name, email, age, income_bracket, customer_tier, risk_profile, city, state, country, registration_date, preferred_categories, is_active, source_system
- **Business Context**: Customer segmentation, personalization, risk assessment

## âš ï¸ **Important Design Note: Circular Dependency in Fake Data Generation**

### **The Pattern**
There exists a **circular dependency** in our fake data generation that's important to understand:

1. **`generate_monthly_data.py`** creates user registration CSV files
2. **`kafka_producer_faker.py`** reads those CSV files to get user IDs
3. **Kafka producer** generates streaming transactions using those same user IDs
4. **Both sources** (batch CSV + streaming Kafka) now reference the same users

### **Why This Exists**
- **Demo purposes**: Ensures referential integrity for dimensional modeling
- **Realistic simulation**: Shows how different data sources can share common entities
- **Testing**: Validates that our dimensional model can handle unified user data

### **Real-World Equivalent**
In production, this would typically be handled by:
- **Master Data Management (MDM)** systems
- **Identity resolution** services
- **Data governance** processes
- **Cross-system** user ID synchronization

### **Current Implementation**
```python
# In kafka_producer_faker.py
def load_user_ids_from_csv() -> list[str]:
    """Load user IDs from the generated user registration CSV files"""
    # This creates the circular dependency for demo purposes
    user_files = list(incoming_dir.glob("user_registrations_*.csv"))
    df = pl.read_csv(latest_file)
    user_ids = df["user_id"].to_list()
    return user_ids
```

### **Alternative Approaches for Production**
1. **Separate user ID pools** for each source
2. **Real-time user registration** events via Kafka
3. **Database lookups** for user validation
4. **API calls** to user management systems

## ğŸ”— **Data Integration Challenges**

### **User ID Mismatch**
- **Challenge**: Ensuring user IDs exist across all sources
- **Current Solution**: Circular dependency in fake data generation
- **Production Solution**: Master data management and identity resolution

### **Update Frequency Mismatch**
- **Challenge**: Batch (monthly) vs real-time (continuous) data
- **Solution**: Incremental processing and real-time streaming

### **Data Quality Variations**
- **Challenge**: Different validation rules per source
- **Solution**: Unified data quality framework in dbt

## ğŸ—ï¸ **Data Flow Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Batch CSV     â”‚    â”‚  Real-time      â”‚    â”‚  User Profile   â”‚
â”‚  Transactions   â”‚    â”‚  Transactions   â”‚    â”‚      CSV        â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ Generated by    â”‚    â”‚ Generated by    â”‚    â”‚ Generated by    â”‚
â”‚ monthly_data.py â”‚    â”‚ kafka_producer  â”‚    â”‚ monthly_data.py â”‚
â”‚                 â”‚    â”‚ (reads CSV)     â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                       â”‚                       â”‚
          â”‚                       â”‚                       â”‚
          â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    dbt Transformation Layer                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Staging       â”‚  â”‚   Intermediate  â”‚  â”‚     Marts       â”‚ â”‚
â”‚  â”‚   Models        â”‚  â”‚     Models      â”‚  â”‚     Models      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                       â”‚                       â”‚
          â”‚                       â”‚                       â”‚
          â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Snowflake Data Warehouse                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ fct_transactionsâ”‚  â”‚fct_daily_trans  â”‚  â”‚ dim_users       â”‚ â”‚
â”‚  â”‚ (Unified)       â”‚  â”‚ (Aggregated)    â”‚  â”‚ (Conformed)     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ˆ **Data Quality & Governance**

### **Quality Metrics**
- **Completeness**: Required fields not null
- **Accuracy**: Amount ranges, valid categories, valid dates
- **Consistency**: User IDs match across sources
- **Timeliness**: Data freshness indicators

### **Governance**
- **Data lineage**: Track data from source to consumption
- **Change management**: Version control for model changes
- **Access control**: Role-based permissions by business unit
- **Audit trail**: Track data modifications and access

## ğŸ¯ **Usage Examples**

### **Demo Setup**
```bash
# 1. Generate user and transaction data
uv run scripts/generate_monthly_data.py --year 2025 --month 8 --users 100 --transactions 500

# 2. Start Kafka producer (will use same user IDs)
uv run kafka/kafka_producer_faker.py

# 3. Run Airflow DAG to ingest data
# 4. Build dbt dimensional model
uv run dbt build --project-dir dbt_sf
```

### **Business Questions Answered**
- Which users have both batch and streaming transactions?
- How do spending patterns differ between data sources?
- What's the user journey across different transaction types?
- Which data sources contribute most to user value?

## ğŸ“š **References**

- [Data Integration Patterns](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/)
- [Master Data Management](https://www.gartner.com/en/information-technology/glossary/master-data-management-mdm)
- [Identity Resolution](https://www.snowflake.com/blog/identity-resolution-data-science/)
